@using Microsoft.CognitiveServices.Speech
@using Microsoft.CognitiveServices.Speech.Audio
@using System.Text.Json
@using WorkDot.Services;
@using WorkDot.Services.Common
@using WorkDot.Services.Models
@using WorkDot.Shared.Plugins
@inject SpeechConfig config
@inject ChatCompletionService chatService
@inject ISnackbar Snackbar


<MudGrid Justify="Justify.Center">
    @if (currentWidget == WidgetType.None)
    {
        <MudItem xs="12" md="12" lg="12">
            <MudText Typo="Typo.h3" Align="Align.Center">Welcome to WorkDot</MudText>
            <MudText Typo="Typo.subtitle1" Align="Align.Center" Class="mud-text-secondary">
                Your AI-powered assistant for Work.
            </MudText>
        </MudItem>
    }
    <MudItem xs="12" md="12" lg="12">
        <div class="d-flex flex-column align-center mt-5">
            <button class="record-button @((isListening) ? "active" : "")"
                    @onclick="StartRecording"
                    disabled="@isListening">
                <MudIcon Icon="@Icons.Material.Filled.MicNone" Title="Mic" Size="Size.Large" />
            </button>
        </div>
        <ChatPlugin Conversation="conversation" />

        @if (isProcessing)
        {
            <PluginSkeleton />
        }
        else if (currentWidget == WidgetType.Plan)
        {
            <PlanPlugin PlanModels="widgetData" />
        }
        else if (currentWidget == WidgetType.Todo)
        {
            <TodoPlugin TodoItems="widgetData" />
        }
    </MudItem>
</MudGrid>


@code {
    private List<ChatMessageModel> conversation = new();
    private bool isListening { get; set; } = false;
    private bool isProcessing { get; set; } = false;
    private WidgetType currentWidget = WidgetType.None;
    private object widgetData = default!;

    public async void StartRecording()
    {
        if (await Permissions.RequestAsync<Permissions.Microphone>() != PermissionStatus.Granted)
        {
            Snackbar.Add("Microphone Permission Denied", Severity.Error);
        }
        else
        {
            isListening = true;
            StateHasChanged();

            using var audioConfig = AudioConfig.FromDefaultMicrophoneInput();
            using var speechRecognizer = new SpeechRecognizer(config, audioConfig);

            try
            {
                var result = await speechRecognizer.RecognizeOnceAsync();
                isListening = false;
                isProcessing = true;
                conversation.Add(new ChatMessageModel() { Source = "U", Text = result.Text });
                StateHasChanged();

                var response = await chatService.GetResponseAsync(result.Text);
                ProcessResponse(response);
                isProcessing = false;
                StateHasChanged();
            }
            catch (System.Exception ex)
            {
                isProcessing = false;
                isListening = false;
                Console.WriteLine(ex);
            }

            StateHasChanged();
        }
    }

    private void ProcessResponse(string response)
    {
        try
        {
            var widgetModel = JsonSerializer.Deserialize<WidgetModel>(response);

            if (widgetModel != null && widgetModel?.Widget != null && widgetModel.Data != null)
            {
                if (Enum.TryParse(widgetModel.Widget.ToString(), out currentWidget))
                {
                    widgetData = widgetModel.Data;
                }
            }
        }
        catch (Exception)
        {
            currentWidget = WidgetType.Chat;
            conversation.Add(new ChatMessageModel() { Source = "W", Text = response });
        }
    }
}